* use terra's GP regression for detrending, rather than legendre series sums.

> not terra's imple,entatio, but the scikit learn implementation seems pretty
> slow. This definitely could be a useful method though (just more complicated
> than necessary)

* make completeness maps (vs δ_inj, P_inj, P_EB, rms_biased)

> Done. We need ~4x better noise RMS across all LCs.
> Note (per discussion w/ Fei Dai) a better metric than RMS is CDPP over 6hrs.
> This is like roughly "pass a 6 hour window over LC. for all the points in
> each window, compute variance. Then compute the mean of the variances".

* check normalization is being done thru *DIVISION* by mean (not subtraction),
b/c division preservers relative fluxes..

> Done

* Possible to high/loss pass filter at ~<2x EB period? (Read Feigelson text & see)

> Butterworth filters are the idea, and they ruin transits (smoothing is too
> powerful, timescales too close -- the sinc(f) of a FT'd box function does not
> play well with Butterworth).

* Email Johan for his code -- more general fourier approach (more expensive
  too).

> Emailed. Tried implemented it too -- works shittily. But my implementation
> may have bugs.

* iteratively whiten via either:
    1. if next bst period is a harmonic of original signal, subtract off new
    legendre fit
    2. scipy.signal: genereate filters based on Fourier representation of LC,
    including all the harmonics, and pass them thru
> barebones implementation done. 

* Iteratively whiten out more frequencies. (Say ~few more) (Nb. this depends on
  whether there are strong periodicities in current redtrended residuals, or
  whether the spot-movement is "pseudo-periodic")

* on iterwhiten plots: add ylabel including iternum (like what's already in fname)
  also show npts & legendredeg on phased fit.

* cut out data within "0.5d regions" of gaps (defined by
    >0.5day space btwn points)
  -> nb. lcmath.find_lc_timegroups(fsaptimes, mingap=mingap) is most relevant.
  -> implement as trim_near_gaps. 
  (nb. does not require "stitching" to get full LC... you can do just over 
  quarters still)

* revise period bounds of iterative whitening to [0.05day, 5xP_EB]

* debug why inum never goes above 2
  > bug: badly formed bracket -> a sqrt was trying to do a negative number and
  nanning

* during iterative whiten, add f(t) filter with mid-level legendre series fit
  (to time-groups), then subsequent subtraction for >~10day trend removal. 

* clear out the TODOs and FIXMEs from the iterative whitening implementation.

* in phase, it does not make sense to limit by npts. It should be the the
  number of phases being folded over! e.g., even w/ 1k points, at a 0.5 day fold
  you have 40 phases. The number of phases should be the determining factor, not
  the number of points (will help with Q0 and Q17)

* revise legendredeg(npts) function... maybe allow sharper even w/ fewer pts, as
  long as within "safe" period bounds (<5x P_EB).

* Similarly, fine-tune the sigma clipping based on the actual RMS across a
    quarter. **If (once detrended+whitened) it's very small, we must allow
    bigger dips (w/out clipping them).**

* lcmath.find_lc_timegroups -- basically an implementation of astrokep's
 find_lightcurve_gaps, already done.

- hard cut on escaping once RMS<0.05% threshold.

* complete tests/assessment of how well iterative whitening looks like it's 
  working (see e.g., how far down you can push in the rms floor)
  (also note that many sharp features seem to fail)

* add find_dip with iterative whitening 

* in recovery, check correct periods, off by 1 orbit in time. (this is b/c 
  min time gets cut. Implement a workaround at some stage, e.g., after
  BLS!)

* reassess completeness (coarse level).
  Result: it's complete enough for ~4Re, ~4-80P_EB period CBPs. (like 50%)

Sun 26 Mar 2017 09:52:32 AM EDT

* Fix injrecovresult_analysis to open whatever pickles are there and use THOSE,
rather than go thru whatever random number seeds are necessary.

> implemented. But now make it a cmd line arg with parser.
> done. we're finally using argparse, and i got the chance to refactor
  run_inj_recov to be wayyy clearer.

Identify highest significance dips in search:
########## 

X compute CDPP 6hr.

> after reading Christiansen's 2012 and Jenkins 2010 (CDPP) description...
this is not worth the effort. They're using some kind of wavelet matched
filter approach, and describe CDPP as a thing that only really can be computed
*sticking to their exact pipeline approach*. better idea is to just stick with
the RMS. 

X make plot to compare success0 and success1 RMSs (once success1 has correct
injrecovresults)

> yes, iterative whitening did help (duh). previously (single iteration) ~20%
of EBs had biased RMS less than 1e-3. with iterative withening, it's ~70%.

* get BLS depth (to compute depth/RMS)

* log recovered BLS depth in the csv files too

* plot: (recovered depth / injected depth) vs RMS (or whatever quantity).

* why do we have negative transit depths recovered?
> because BLS can find them. In a smaller number of cases (~10 of the ~540),
the whitening subtracts the planet signal (notably if it's very big). But the
point is that BLS has no prior for wanting positive depth transits. So 75 of
540 injection recovery tests wind up with NEGATIVE depths (mostly because of
stellar variability producing some kind of coherent signal in the periodogram).
It seems like it would be smarter to have some prior to avoid this.
Like "if the BLS depth for this proposed period is negative... downweight the
power of this period!!"
(Note e.g., the case dipsearchplot/inj/10090246_sapdipsearch_inj_0.00125.png:
primary period gives such a negative depth (i.e. a brightening). But there the
4th period gives the correct dip)

* does the 4th period get the correct recovered depth?
> yes. nthbestperiod gets serialdict, which contains the depths.

* in periodbase/kbls.py: add condition to ignore peaks that give BRIGHTENINGS
(rather than dimmings), i.e. get nbestpeaks that are POSITIVE.
> no, not in kbls.py. Reason: we need this to be happening in serial pfinder.
Like, the nbestperiods can only be selected based off the full periodogram (not
the depth info), because we don't know the depths for the nbestperiods (because
`eebls` only gives us the depths across frequency chunks).

This can be easily cut in post-processing -- just ignore anything with negative
depth -- but the issue is that we then aren't getting the full nbestpeaks for
each star (we'd be getting a few less -- like 15% less, since the problem is
~70 out of 500).

That's boarderline.

So to solve:
  * take e.g., 10 nbestperiods from parallel_pfind, and go thru them serially
  discarding anything that gives negative depths. Lets you go deeper into the
  periodogram

> done.

* bugfix:
(sci) luke@crispy:~/Dropbox/proj/cbp/src$ Traceback (most recent call last):
  File "run_inj_recov.py", line 522, in <module>
    injrecov_test1(103, stage='dipsearch', inj=True, ds=True, whitened=True)
  File "run_inj_recov.py", line 438, in injrecov_test1
    allq = ir.find_dips(lcd, allq, method='bls')
  File "/home/luke/Dropbox/proj/cbp/src/inj_recov.py", line 1930, in find_dips
    φ_egr = φ_bin[egrbinind]
IndexError: index 393 is out of bounds for axis 0 with size 392

[1]+  Exit 1                  python run_inj_recov.py > LOGS/completeness_test_170320_5.txt

Mon 27 Mar 2017 09:20:33 AM EDT

* after runs are complete, parse LOGS for LOGERRORs, e.g.,
      LOGERROR('ingbinind from eebls.f shorter than it should be.')
      LOGERROR('Hard-setting ingress and egress phases.')
  (that bug that I hard-coded a hacky-exception for, because I don't understand
  it)
  > put it to summary.txt (like: "there were X logerrors: (and list all of em)"

* refactor (e.g., on a new git branch) to take out pieces of code that are not
  and will not be used any more.
  -> tag current version as v0.1

* make injection actually optional in main (so that you can just run whitening
  and dipfinding search over all the KEBC stars)

*  fix retrieve_next_lc logic for repeats to work (in the style of: you have
the pickles, now you want the plots that you messed up earlier) 
        lcd, lcflag = ir.retrieve_next_lc()

* fix the order of dips in dipsearchplot to be correct...
  also fix the ylims
  also fix the dashed red lines shown on periodogram

Thu 30 Mar 2017 11:06:36 AM EDT

* turn ur existing completeness map data into a "heatmap" (like shade in where
  you can detect things, and where you can't). then show ur candidates on that
  map.

* add baseline, SNR_inj_pf, SNR_rec_pf to result csvs

* Write something that lets you regenerate, from pkls:
  X injrecov summary csv files
  > LOL i already did this. that's the `-p` option.
  X recov summary csv files
  > added option for this

* Sort discovered candidates by depth/RMS * sqrt(obs baseline)... or something else?
  * Make plots to see how clean this is (scatter, then completeness maps)

* debug:
(sci) luke@crispy:~/Dropbox/proj/cbp/src$ Traceback (most recent call last):
File "run_the_machine.py", line 146, in recov irp.whitenedplot_6row(lcd,
ap='sap', stage=stage, inj=inj) File
"/home/luke/Dropbox/proj/cbp/src/inj_recov_plots.py", line 549, in
whitenedplot_6row f.savefig(savedir+plotname, dpi=300) File
"/home/luke/Dropbox/miniconda3/envs/sci/lib/python3.5/site-packages/matplotlib/figure.py",
line 1565, in savefig self.canvas.print_figure(*args, **kwargs) File
"/home/luke/Dropbox/miniconda3/envs/sci/lib/python3.5/site-packages/matplotlib/backends/backend_qt5agg.py",
line 196, in print_figure FigureCanvasAgg.print_figure(self, *args, **kwargs)
File
"/home/luke/Dropbox/miniconda3/envs/sci/lib/python3.5/site-packages/matplotlib/backend_bases.py",
line 2232, in print_figure **kwargs) File
"/home/luke/Dropbox/miniconda3/envs/sci/lib/python3.5/site-packages/matplotlib/backends/backend_agg.py",
line 527, in print_png FigureCanvasAgg.draw(self) File
"/home/luke/Dropbox/miniconda3/envs/sci/lib/python3.5/site-packages/matplotlib/backends/backend_agg.py",
line 474, in draw self.figure.draw(self.renderer) File
"/home/luke/Dropbox/miniconda3/envs/sci/lib/python3.5/site-packages/matplotlib/artist.py",
line 61, in draw_wrapper draw(artist, renderer, *args, **kwargs) File
"/home/luke/Dropbox/miniconda3/envs/sci/lib/python3.5/site-packages/matplotlib/figure.py",
line 1159, in draw func(*args) File
"/home/luke/Dropbox/miniconda3/envs/sci/lib/python3.5/site-packages/matplotlib/artist.py",
line 61, in draw_wrapper draw(artist, renderer, *args, **kwargs) File
"/home/luke/Dropbox/miniconda3/envs/sci/lib/python3.5/site-packages/matplotlib/axes/_base.py",
line 2324, in draw a.draw(renderer) File
"/home/luke/Dropbox/miniconda3/envs/sci/lib/python3.5/site-packages/matplotlib/artist.py",
line 61, in draw_wrapper draw(artist, renderer, *args, **kwargs) File
"/home/luke/Dropbox/miniconda3/envs/sci/lib/python3.5/site-packages/matplotlib/text.py",
line 757, in draw raise ValueError("posx and posy should be finite values")
ValueError: posx and posy should be finite values

During handling of the above exception, another exception occurred:

Traceback (most recent call last): File "run_the_machine.py", line 404, in
<module> nwhiten_max=10, nwhiten_min=1, rms_floor=5e-4) File
"run_the_machine.py", line 149, in recov str(keplerid))) NameError: name
'keplerid' is not defined

* check why output depths are still negative (even post-correction... this is
likely something in the postprocessing? (else it's that I actually do need to
redo all my injection/recovery runs)
  > ya, it's fine for real_search. Just the output from injection/recovery is
  before I made this change.

* bugfix control flow when whitened 6row plot err

* write an analog of this that summarizes result from realsearch. (in
run_the_machine) irra.summarize_realsearch_result(). (Now using pf_SNR)

* Um. The Jenkins matched-filter approach is VERY DIFFERENT on the surface from
e.g., BLS. Are they equivalent? Which is better?  

>  Ya, it is. Chelsea Huang nominally ran a comparison back in 2012 (see Xu
Huang et al ApJ with the HAT group). The result is that BLS is a bit more
complete at short periods, for medium sized things. But the Kepler pipeline
does better at longer periods with smaller things. This means it might be a
good idea to look closely at the Kepler pipeline's methods, at least when
processing Kepler data!

Fri 31 Mar 2017 08:45:28 AM EDT

* Filter sorted results by isclose to multiples of P_rec_by_P_EB

* debug:

(sci) luke@crispy:~/Dropbox/proj/cbp/src$ WARNING: File may have been truncated: actual file length (49152) is smaller than the expected size (440640) [astropy.io.fits.file]
Traceback (most recent call last):
  File "/home/luke/Dropbox/miniconda3/envs/sci/lib/python3.5/site-packages/astropy/utils/decorators.py", line 496, in __get__
    return obj.__dict__[self._key]
KeyError: 'data'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run_the_machine.py", line 407, in <module>
    nwhiten_max=10, nwhiten_min=1, rms_floor=5e-4)
  File "run_the_machine.py", line 76, in recov
    blacklist=blacklist)
  File "/home/luke/Dropbox/proj/cbp/src/inj_recov.py", line 473, in retrieve_next_lc
    rd, errflag = get_all_quarters_lc_data(this_id)
  File "/home/luke/Dropbox/proj/cbp/src/inj_recov.py", line 530, in get_all_quarters_lc_data
    lcd = astrokep.read_kepler_fitslc(fits_path)
  File "/home/luke/Dropbox/miniconda3/envs/sci/lib/python3.5/site-packages/astrobase-0.1.0-py3.5.egg/astrobase/astrokep.py", line 226, in read_kepler_fitslc
    lchdr, lcdata = hdulist[1].header, hdulist[1].data
  File "/home/luke/Dropbox/miniconda3/envs/sci/lib/python3.5/site-packages/astropy/utils/decorators.py", line 498, in __get__
    val = self.fget(obj)
  File "/home/luke/Dropbox/miniconda3/envs/sci/lib/python3.5/site-packages/astropy/io/fits/hdu/table.py", line 404, in data
    data = self._get_tbdata()
  File "/home/luke/Dropbox/miniconda3/envs/sci/lib/python3.5/site-packages/astropy/io/fits/hdu/table.py", line 171, in _get_tbdata
    self._data_offset)
  File "/home/luke/Dropbox/miniconda3/envs/sci/lib/python3.5/site-packages/astropy/io/fits/hdu/base.py", line 568, in _get_raw_data
    return self._file.readarray(offset=offset, dtype=code, shape=shape)
  File "/home/luke/Dropbox/miniconda3/envs/sci/lib/python3.5/site-packages/astropy/io/fits/file.py", line 283, in readarray
    buffer=self._mmap)
TypeError: buffer is too small for requested array
(sci) luke@crispy:~/Dropbox/proj/cbp/src$ 
[1]+  Exit 1                  python run_the_machine.py -frd > LOGS/findrealdips_170327_6.txt


* Discuss difference btwn things doing things the right way vs hacks with APW.
The basic take-away is that you need to do things in line with your goals.
E.g., if you just want to discover things, with statistical precision on the
back-burner, yes, something like sigmaclipping is OK.  But if you want to do
population inference on CBP occurrence rate about short-period binaries, you
need (at the very minimum) to understand your completeness. However doing so is
tricky -- you need to know that your injected signals are truly what you would
see. You might also need to debias your sample in other ways -- that was a
major idea in Armstrong's approach. (It is also why reading his THESIS, which
is presumably more verbose than the paper, could be worthwhile)

* Read Armstrong's _thesis_, b/c it might have more info than the paper (e.g.,
about sample selection, and short period binaries, and reasons for specific
methods, and whether he thinks he was complete for short-period binaries)

Sat 01 Apr 2017 05:50:47 PM EDT

Targetted reading:

* What parameters of the contact EBs must be known to make an inference about a
putative population of CBPs about them? 
(read DFM's code & the long period planet paper math)
> notes are written out; 170401_detection_efficiency_OOM.pdf

* Redefined motivation: if 10% of stars in short-period EB sample had Rp>4Re,
P_CPB=1-200d planets, we'd detect (Q_geom=0.1,Q_completeness=0.5)
1500*0.1*0.1*0.5 = 7.5 of em. If the Q_geom and Q_completeness estimates are
good, detecting <1 of them would mean an occurrence ~<3%(?) per star in the
short-period EB sample (in Rp=4-10Re vol, P_CBP=1-200d). (N.b. the (?) is
because Poisson statistics are the relevant thing there.) In other words, also
significantly lower than the standard star rate.  This result is an empirical
extension of Armstrong et al (2014)'s work on semi-detached and detached
binaries, into the contact binary regime. It is not surprising (if indeed the
search gives a null result): the majority (e.g., Tokovinin+ 2008) of contact
binaries have tertiary companions, and the very fact of being a contact binary
may preclude stability of CBPs during migration, at least in regimes observable
by transit surveys (the 3 recent theory papers, + Wenrui Xu's evection
resonance stuff).

* What parameters are actually known?  
(read Armstrong 2013 catalog paper; see how much he trusts the contact binary
parameters, and why) 
(read Ch 6 of Hilditch (emph on Ch6.4))
> notes are in lib/summarized_papers.txt

* What parameters can be estimated? (read the Rucinski papers, most notably
2006 Luminosity function, 2001 amplitude and mass ratio distributions, and
~2012 DDO Close Binary Spectroscopic Program)
> notes are in lib/summarized_papers.txt


##############################
##############################

Mon 03 Apr 2017 09:53:00 PM EDT

* make easiest disk space improvements. are we massively copying big things?
does each pkl need ~30Mb?
> Well, those with many iterations had ~100Mb. I implemented something to
drop the intermediate periodograms (the ones that i never look at) (although
this means backwards incompatibility w/ the 3row diagnostic)

> At 30Mb/ea, ~1600 LCs means main run produces 48Gb of data.
That's FINE. (Even at ~60Mb/ea, accounting for allq, 100Gb is ok).

> For injection recovery, we actually need far less data for production level
runs. Namely, we just need the planet and stars params and whether the injected
dip was found. So we can do ~20k inj/recovs, and keep maybe the first ~50
pickles or something like that. (As sanity checks). We could also only produce
plots for those 50. After that, proceed by just keeping track of some waaaay
lighter-weight thing.

This is OFC risky b/c if there's a mistake in the information that is saved
(like "oh hey, I actually wanted all those inj/recov periodograms!") then it
all needs to be recomputed. However for the number of inj/recovs we're talking
about, we just cannot store the full periodograms, and similarly there is not
much point in holding onto the full LC. (Obv different for the real search).


Tue 04 Apr 2017 05:17:19 PM EDT

Easiest speed improvements:
* do not make diagnostic plots until after pf-SNR is computed. If it's <3, we
  will not look at the LC, so do not make the plots.

*debug:
(sci) luke@crispy:~/Dropbox/proj/cbp/src$ Traceback (most recent call last):
  File "run_the_machine.py", line 407, in <module>
    lcd = get_lcd(stage='dtr', inj=False)
  File "run_the_machine.py", line 93, in recov
    if 'realsearch' in stage:
  File "/home/luke/Dropbox/proj/cbp/src/inj_recov.py", line 651, in detrend_allquarters
    inj=inj)
  File "/home/luke/Dropbox/proj/cbp/src/inj_recov.py", line 747, in detrend_lightcurve
    sel = (tgtimes > npmin(tgtimes)+mingap) & \
  File "/home/luke/Dropbox/miniconda3/envs/sci/lib/python3.5/site-packages/numpy/core/fromnumeric.py", line 2359, in amin
    out=out, keepdims=keepdims)
  File "/home/luke/Dropbox/miniconda3/envs/sci/lib/python3.5/site-packages/numpy/core/_methods.py", line 29, in _amin
    return umr_minimum(a, axis, None, out, keepdims)
ValueError: zero-size array to reduction operation minimum which has no identity

* v1.0 tag
